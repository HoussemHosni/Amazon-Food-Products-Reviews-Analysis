{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Food Products Reviews Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source : https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\n",
    "\n",
    "Reviews.csv: Pulled from the corresponding SQLite table named Reviews in database.sqlite\n",
    "database.sqlite: Contains the table 'Reviews'\n",
    "\n",
    "Data includes:\n",
    "- Reviews from Oct 1999 - Oct 2012\n",
    "- 568,454 reviews\n",
    "- 256,059 users\n",
    "- 74,258 products\n",
    "- 260 users with > 50 reviews\n",
    "\n",
    "Attribute Information:\n",
    "1. Id\n",
    "2. ProductId - unique identifier for the product\n",
    "3. UserId - unqiue identifier for the user\n",
    "4. ProfileName\n",
    "5. HelpfulnessNumerator - number of users who found the review helpful\n",
    "6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "7. Score - rating between 1 and 5\n",
    "8. Time - timestamp for the review\n",
    "9. Summary - brief summary of the review\n",
    "10. Text - text of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective:\n",
    "Given a review, we'll try to determine whether the review is positive (Rating of 4 or 5) or negative (rating of 1 or 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data (from SQLite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available in two forms\n",
    "1. .csv file\n",
    "2. SQLite Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLite dataset is easier to query and visualise efficiently. So we are going to load our data from SQLite and not .csv. \n",
    "Here as we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If rating is 4 or 5 then the recommendation wil be set to \"positive\". Otherwise (1 or 2), it will be set to \"negative\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a connection to our database.\n",
    "con = sqlite3.connect(\"database.sqlite\")\n",
    "# Now we can start fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filtered_dataset = pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE score != 3\n",
    "\"\"\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignsentiment(x):\n",
    "    if x < 3:\n",
    "        return \"negative\"\n",
    "    else: return \"positive\"\n",
    "# Let's map our function on score column\n",
    "filtered_dataset[\"Score\"] = filtered_dataset[\"Score\"].map(assignsentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset.head()\n",
    "# Scores have been replaced by sentiments (+ or -)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning : Deduplication (removing duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE score!=3 AND UserId=\"AR5J8UI46CURR\"\n",
    "ORDER BY ProductId\n",
    "\"\"\"    \n",
    ",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>320691</td>\n",
       "      <td>B000CQ26E0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1187740800</td>\n",
       "      <td>Fast, Easy and organic</td>\n",
       "      <td>For speed and wholesome goodness, Annie's can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>468954</td>\n",
       "      <td>B004DMGQKE</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Awesome service and great products</td>\n",
       "      <td>We sent this product as a gift to my husband's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId                      ProfileName  \\\n",
       "0  320691  B000CQ26E0  ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "1       3  B000LQOCH0  ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "2  468954  B004DMGQKE  ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     0                       0      4  1187740800   \n",
       "1                     1                       1      4  1219017600   \n",
       "2                     0                       0      5  1351209600   \n",
       "\n",
       "                              Summary  \\\n",
       "0              Fast, Easy and organic   \n",
       "1               \"Delight\" says it all   \n",
       "2  Awesome service and great products   \n",
       "\n",
       "                                                Text  \n",
       "0  For speed and wholesome goodness, Annie's can ...  \n",
       "1  This is a confection that has been around a fe...  \n",
       "2  We sent this product as a gift to my husband's...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query(\"\"\"SELECT *\n",
    "From Reviews\n",
    "WHERE score !=3 AND UserId=\"ABXLMWJIXXAIN\"\n",
    "ORDER BY ProductId\n",
    "\"\"\"\n",
    ",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>483943</td>\n",
       "      <td>B003XDH6M6</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1315699200</td>\n",
       "      <td>Yum, tasty</td>\n",
       "      <td>Didn't know what to expect from Newman's own l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId                    ProfileName  \\\n",
       "0  483943  B003XDH6M6  A1UQRSCLF8GW1T  Michael D. Bigham \"M. Wassir\"   \n",
       "1       5  B006K2ZZ7K  A1UQRSCLF8GW1T  Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     0                       0      5  1315699200   \n",
       "1                     0                       0      5  1350777600   \n",
       "\n",
       "       Summary                                               Text  \n",
       "0   Yum, tasty  Didn't know what to expect from Newman's own l...  \n",
       "1  Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A1UQRSCLF8GW1T\n",
    "pd.read_sql_query(\"\"\" SELECT *\n",
    "FROM Reviews\n",
    "WHERE score != 3 AND UserId = \"A1UQRSCLF8GW1T\"\n",
    "ORDER BY ProductId    \n",
    "\"\"\"    \n",
    ",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dataset = filtered_dataset.sort_values(by = [\"ProductId\"],axis=0,ascending=True,inplace = False, kind=\"quicksort\",na_position=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = sorted_dataset.drop_duplicates(subset = {\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep = \"first\", inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.25890143662969\n"
     ]
    }
   ],
   "source": [
    "#How much % of the original data we have got in final_dataset\n",
    "print((final_dataset.shape[0]/sorted_dataset.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId              ProfileName  \\\n",
       "0  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "1  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       2      4  1212883200   \n",
       "1                     3                       1      5  1224892800   \n",
       "\n",
       "                                        Summary  \\\n",
       "0  Pure cocoa taste with crunchy almonds inside   \n",
       "1             Bought This for My Son at College   \n",
       "\n",
       "                                                Text  \n",
       "0  It was almost a 'love at first bite' - the per...  \n",
       "1  My son loves spaghetti so I didn't hesitate or...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HelpfulnessNumerator can not logically be greater than HelpfulnessDenominator\n",
    "# So let's remove data points where HelpfulnessNumerator > HelpfulnessDenominator\n",
    "pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE score !=3 AND HelpfulnessNumerator > HelpfulnessDenominator\n",
    "\"\"\"\n",
    ",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = final_dataset[final_dataset[\"HelpfulnessNumerator\"]<=final_dataset[\"HelpfulnessDenominator\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    307061\n",
       "negative     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many positive and negative reviews are present inour dataset\n",
    "final_dataset[\"Score\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing: Stemming, stop-words removal, lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go on further with analysis and prediction models our Data still needs some text preprocessing and cleaning.\n",
    "In this pre-processing phase we pursue the following steps:\n",
    "1. Removing the html tags\n",
    "2. Remove any punctuations or limited set of special characters like , or . or #, !, ? etc...\n",
    "3. Check if the word is made up of english letters and is not alpha-numeric\n",
    "4. Check to see if the length of the word is greater than 2 (there is no adjective in 2-letters)\n",
    "5. Convert the word to lowercase\n",
    "6. Remove Stopwords\n",
    "7. Finally Snowball Stemming the word (better than Porter Stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's import the packages that we will use during the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "stopwrd = set(stopwords.words(\"english\")) #set of stopwords\n",
    "snoball = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n",
    "# Let's define two important functions :cleanhtml and cleanpunc\n",
    "def cleanhtml(sentence): # function to clean the word of any html tag\n",
    "    htmlpattern = re.compile('<.*?>')\n",
    "    cleantext = re.sub(htmlpattern,' ',sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence): # function to clean the word of any punctuation or special chars\n",
    "    cleaned = re.sub(r'[!|?|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|/|\\|(|)|]',r' ',cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ourselves', 'in', 'when', 'own', 'him', 'very', 'did', \"it's\", 'with', 'between', 'our', 'as', 'does', 've', 'for', 'my', 'at', 'will', 'while', 'herself', \"haven't\", 'he', 'your', 'itself', 'some', 'into', 'didn', 'off', 'were', 'are', \"hadn't\", 'am', \"you've\", 'weren', 'but', 'can', 'should', 'all', 'hasn', 'now', 'was', \"wasn't\", 'aren', \"should've\", \"mightn't\", \"she's\", 'such', \"you'd\", 'because', 'how', 'down', 'then', 'than', 'who', 'the', 'having', 'won', 'too', 'those', 'y', 'out', 'of', 't', 'shouldn', 'his', 'hadn', 'do', 'being', 'mustn', \"mustn't\", 'don', 'more', 'if', \"weren't\", 'hers', 'same', 'not', \"isn't\", 'has', 'yours', 'm', 'we', 'ma', 'doesn', 'is', 'under', 'both', \"shan't\", 'isn', 'during', 's', 'you', 'wouldn', 'through', 'i', 'to', 'mightn', 'himself', 'she', 'they', 'had', 'an', 'against', 'until', \"hasn't\", 'from', 'be', 'yourself', 'wasn', \"won't\", 'again', 're', 'myself', 'haven', 'll', \"shouldn't\", 'above', 'me', 'it', 'each', 'below', 'couldn', 'just', 'their', 'needn', 'ain', 'by', 'no', 'ours', 'and', 'her', \"couldn't\", 'or', 'most', 'only', 'which', \"didn't\", 'other', \"needn't\", 'that', 'theirs', 'once', 'here', \"aren't\", 'o', 'after', 'what', \"you'll\", 'this', \"don't\", \"that'll\", 'these', 'themselves', 'where', 'about', 'so', \"wouldn't\", 'd', 'been', 'them', 'few', 'there', 'its', 'doing', 'have', \"doesn't\", 'shan', 'nor', 'yourselves', 'up', 'a', 'whom', 'any', 'over', 'further', 'before', 'on', 'why', \"you're\"}\n",
      "--------------------------------\n",
      "delici\n"
     ]
    }
   ],
   "source": [
    "print(stopwrd)\n",
    "print(\"--------------------------------\")\n",
    "print(snoball.stem(\"delicious\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  663.0993470728447 seconds\n"
     ]
    }
   ],
   "source": [
    "#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n",
    "# this code takes a while to run as it needs to run on 500k sentences.\n",
    "import time\n",
    "start_time = time.clock()\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in final_dataset['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\n",
    "    sent=cleanhtml(sent) # remove HTMl tags\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stopwrd):\n",
    "                    s=(snoball.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (final_dataset['Score'].values)[i] == 'positive': \n",
    "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
    "                    if(final_dataset['Score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    #print(filtered_sentence)\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words  \n",
    "    final_string.append(str1)\n",
    "    i+=1\n",
    "print(\"It took \",time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset[\"CleanedText\"] = final_string\n",
    "final_dataset['CleanedText']=final_dataset['CleanedText'].str.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### our reviews (\"Text\") column in our dataset has been cleaned! \n",
    "#### Our reviews are cleaned and stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bought sever vital can dog food product found good qualiti product look like stew process meat smell better labrador finicki appreci product better'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset[\"CleanedText\"][0] # New Text(review) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset[\"Text\"][0] # Old Text(review) column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's save our final cleaned dataset to an SQLite table so that it can be leveraged in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"final.sqlite\")\n",
    "c=conn.cursor()\n",
    "conn.text_factory = str\n",
    "final_dataset.to_sql('Reviews', conn,  schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "connn = sqlite3.connect(\"final.sqlite\")\n",
    "import pandas as pd\n",
    "final_dataset = pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "\"\"\",connn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "BoW_sparse_matrix = count_vect.fit_transform(final_dataset[\"CleanedText\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 71624)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW_sparse_matrix.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(71624):\n",
    "    if BoW_sparse_matrix[1,i]!=0:\n",
    "        count +=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see our BoW matrix is extremely sparse !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (364171, 71624)\n",
      "the number of unique words  71624\n"
     ]
    }
   ],
   "source": [
    "print(\"the type of count vectorizer \",type(BoW_sparse_matrix)) # sparse matrix storage/representation technique\n",
    "print(\"the shape of out text BOW vectorizer \",BoW_sparse_matrix.get_shape())\n",
    "print(\"the number of unique words \", BoW_sparse_matrix.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Grams and n-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Positive Words :  [(b'like', 139429), (b'tast', 129047), (b'good', 112766), (b'flavor', 109624), (b'love', 107357), (b'use', 103888), (b'great', 103870), (b'one', 96726), (b'product', 91033), (b'tri', 86791), (b'tea', 83888), (b'coffe', 78814), (b'make', 75107), (b'get', 72125), (b'food', 64802), (b'would', 55568), (b'time', 55264), (b'buy', 54198), (b'realli', 52715), (b'eat', 52004)]\n",
      "Most Common Negative Words :  [(b'tast', 34585), (b'like', 32330), (b'product', 28218), (b'one', 20569), (b'flavor', 19575), (b'would', 17972), (b'tri', 17753), (b'use', 15302), (b'good', 15041), (b'coffe', 14716), (b'get', 13786), (b'buy', 13752), (b'order', 12871), (b'food', 12754), (b'dont', 11877), (b'tea', 11665), (b'even', 11085), (b'box', 10844), (b'amazon', 10073), (b'make', 9840)]\n",
      "It took  10.572324116275013 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "freq_dist_positive=nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative=nltk.FreqDist(all_negative_words)\n",
    "print(\"Most Common Positive Words : \",freq_dist_positive.most_common(20))\n",
    "print(\"Most Common Negative Words : \",freq_dist_negative.most_common(20))\n",
    "print(\"It took \",time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that bi-grams are efficient in our case because of the many common words between negative and positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.092445783170895  seconds\n"
     ]
    }
   ],
   "source": [
    "# bi-grams , tri-grams and n-grams\n",
    "import time\n",
    "start_time = time.clock()\n",
    "count_vect2 = CountVectorizer(ngram_range=(1,2))\n",
    "Final_bigrams = count_vect2.fit_transform(final_dataset[\"CleanedText\"].values)\n",
    "print(time.clock()-start_time,\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2923725)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_bigrams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Final_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.91368483393458  seconds\n",
      "The type of countvectorizer:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "The size of tfidf vectorizer:  (364171, 2923725)\n",
      "The number of unique uni-grams and bi-grams in our dataset:  2923725\n"
     ]
    }
   ],
   "source": [
    "time_start = time.clock()\n",
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tfidf = tf_idf_vect.fit_transform(final_dataset[\"CleanedText\"].values)\n",
    "print(time.clock()-time_start,\" seconds\")\n",
    "print(\"The type of countvectorizer: \",type(final_tfidf))\n",
    "print(\"The size of tfidf vectorizer: \", final_tfidf.get_shape())\n",
    "print(\"The number of unique uni-grams and bi-grams in our dataset: \",final_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2923725\n"
     ]
    }
   ],
   "source": [
    "features = tf_idf_vect.get_feature_names()\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa magazin',\n",
       " 'aaa perfect',\n",
       " 'aaa plus',\n",
       " 'aaa rate',\n",
       " 'aaa spelt',\n",
       " 'aaa tue',\n",
       " 'aaaa',\n",
       " 'aaaaa',\n",
       " 'aaaaa kid',\n",
       " 'aaaaa start']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Let's see the 10th line (10th review) \n",
    "print(final_tfidf[10,:].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poem</td>\n",
       "      <td>0.383150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>handmot</td>\n",
       "      <td>0.239019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>throughout school</td>\n",
       "      <td>0.239019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>invent poem</td>\n",
       "      <td>0.239019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>learn poem</td>\n",
       "      <td>0.239019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>like handmot</td>\n",
       "      <td>0.239019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>handmot invent</td>\n",
       "      <td>0.239019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>poem throughout</td>\n",
       "      <td>0.239019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>way children</td>\n",
       "      <td>0.231628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>learn month</td>\n",
       "      <td>0.226384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>children learn</td>\n",
       "      <td>0.226384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>learn</td>\n",
       "      <td>0.211163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>year learn</td>\n",
       "      <td>0.200008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>school year</td>\n",
       "      <td>0.183521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fun way</td>\n",
       "      <td>0.175560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>month year</td>\n",
       "      <td>0.163934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>year like</td>\n",
       "      <td>0.155817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>invent</td>\n",
       "      <td>0.144460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>year</td>\n",
       "      <td>0.124121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>throughout</td>\n",
       "      <td>0.121022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>school</td>\n",
       "      <td>0.112385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>children</td>\n",
       "      <td>0.107703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fun</td>\n",
       "      <td>0.103013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>month</td>\n",
       "      <td>0.073884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>way</td>\n",
       "      <td>0.064301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature     tfidf\n",
       "0                poem  0.383150\n",
       "1             handmot  0.239019\n",
       "2   throughout school  0.239019\n",
       "3         invent poem  0.239019\n",
       "4          learn poem  0.239019\n",
       "5        like handmot  0.239019\n",
       "6      handmot invent  0.239019\n",
       "7     poem throughout  0.239019\n",
       "8        way children  0.231628\n",
       "9         learn month  0.226384\n",
       "10     children learn  0.226384\n",
       "11              learn  0.211163\n",
       "12         year learn  0.200008\n",
       "13        school year  0.183521\n",
       "14            fun way  0.175560\n",
       "15         month year  0.163934\n",
       "16          year like  0.155817\n",
       "17             invent  0.144460\n",
       "18               year  0.124121\n",
       "19         throughout  0.121022\n",
       "20             school  0.112385\n",
       "21           children  0.107703\n",
       "22                fun  0.103013\n",
       "23              month  0.073884\n",
       "24                way  0.064301"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://buhrmann.github.io/tfidf-analysis.html\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "# Convert the second line(review) of our final_tfidf to an array then apply top_tfidf_feats\n",
    "top_tfidf = top_tfidf_feats(final_tfidf[2,:].toarray()[0],features,25)\n",
    "top_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Google-News based Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.97020279370645  seconds!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "# We are going to use google news Word2Vectors\n",
    "# We load google word2vec model\n",
    "time_start = time.clock()\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print(time.clock()-time_start,\" seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.03759766e-02, -4.85839844e-02, -1.26953125e-01, -1.08886719e-01,\n",
       "        3.02734375e-02,  2.23388672e-02, -7.32421875e-04,  9.91210938e-02,\n",
       "        8.20312500e-02,  1.80664062e-02,  6.59179688e-02,  1.04492188e-01,\n",
       "       -1.00097656e-01, -1.21093750e-01, -1.06933594e-01,  9.52148438e-02,\n",
       "        7.56835938e-02,  1.96289062e-01,  1.37695312e-01, -4.95605469e-02,\n",
       "       -9.22851562e-02, -6.64062500e-02, -7.47070312e-02, -1.04003906e-01,\n",
       "        2.46582031e-02,  1.08398438e-01,  6.68945312e-02,  1.03515625e-01,\n",
       "        1.01074219e-01, -5.00488281e-02,  4.24804688e-02, -2.25585938e-01,\n",
       "       -3.56445312e-02,  5.95703125e-02,  1.44531250e-01,  1.53320312e-01,\n",
       "        2.01171875e-01, -4.71191406e-02, -1.59912109e-02, -2.62451172e-02,\n",
       "        1.27929688e-01,  1.78710938e-01, -8.44726562e-02,  1.39648438e-01,\n",
       "       -1.87500000e-01, -1.85546875e-01,  1.02539062e-01,  6.25000000e-02,\n",
       "       -6.98242188e-02,  2.87109375e-01, -2.29492188e-02,  3.39355469e-02,\n",
       "       -3.34472656e-02,  3.19824219e-02,  1.14746094e-01,  1.08886719e-01,\n",
       "        6.68945312e-02,  5.88378906e-02,  1.86523438e-01, -1.33789062e-01,\n",
       "       -1.26953125e-01,  4.02832031e-02, -2.59765625e-01, -3.90625000e-02,\n",
       "       -9.22851562e-02,  1.11816406e-01,  4.80957031e-02, -6.86645508e-03,\n",
       "        1.08886719e-01, -2.12402344e-02, -1.05468750e-01, -7.81250000e-02,\n",
       "        2.38281250e-01, -8.30078125e-02, -9.32617188e-02,  4.44335938e-02,\n",
       "        8.59375000e-02,  1.02050781e-01, -7.72094727e-03, -9.37500000e-02,\n",
       "       -1.19140625e-01, -4.12597656e-02,  8.72802734e-03,  1.45507812e-01,\n",
       "       -1.24023438e-01, -5.71289062e-02, -5.88378906e-02,  3.61328125e-01,\n",
       "        3.06396484e-02,  7.61718750e-02,  2.89062500e-01, -4.88281250e-02,\n",
       "       -1.72851562e-01, -1.50390625e-01, -1.52343750e-01, -2.25585938e-01,\n",
       "       -1.19628906e-01, -4.61425781e-02,  7.61718750e-02,  5.49316406e-03,\n",
       "       -1.40991211e-02, -7.32421875e-02,  5.27954102e-03,  1.09375000e-01,\n",
       "       -1.37329102e-02, -4.73632812e-02,  1.41601562e-02,  1.80664062e-01,\n",
       "        2.61718750e-01, -1.26953125e-01, -4.19921875e-02, -1.35742188e-01,\n",
       "       -2.43164062e-01, -9.91210938e-02,  2.40234375e-01,  5.98144531e-02,\n",
       "        5.44433594e-02, -2.23388672e-02,  2.53906250e-01,  1.25976562e-01,\n",
       "       -1.26953125e-01, -3.91006470e-05, -5.61523438e-02,  1.85546875e-01,\n",
       "       -2.66113281e-02, -1.05957031e-01, -2.39257812e-01,  9.71679688e-02,\n",
       "        5.76171875e-02,  6.98242188e-02, -1.25976562e-01, -3.44238281e-02,\n",
       "        1.13769531e-01, -1.82342529e-03, -1.93786621e-03,  1.25976562e-01,\n",
       "       -6.53076172e-03,  1.85546875e-01,  1.71875000e-01, -3.68652344e-02,\n",
       "       -2.84423828e-02, -2.43164062e-01,  1.80053711e-03,  3.10058594e-02,\n",
       "        9.17968750e-02, -2.22167969e-02,  1.35742188e-01, -7.59887695e-03,\n",
       "        2.97851562e-02,  7.27539062e-02,  1.04492188e-01,  4.83398438e-02,\n",
       "       -5.12695312e-02,  2.55859375e-01, -2.39257812e-01,  4.85839844e-02,\n",
       "        2.68554688e-03, -2.63671875e-02,  7.03125000e-02,  6.77490234e-03,\n",
       "       -1.69921875e-01,  5.41992188e-02, -1.31835938e-01, -8.69750977e-04,\n",
       "        1.85546875e-01,  2.84423828e-02, -7.29370117e-03, -2.08007812e-01,\n",
       "        1.19628906e-01, -5.12695312e-02, -1.19140625e-01, -1.33789062e-01,\n",
       "        1.92382812e-01, -1.24511719e-01, -4.63867188e-02,  5.39550781e-02,\n",
       "        1.71875000e-01,  4.10156250e-02, -8.93554688e-02, -7.32421875e-03,\n",
       "       -8.78906250e-02, -5.66406250e-02, -8.39843750e-02, -1.60156250e-01,\n",
       "       -3.46679688e-02, -2.27539062e-01,  9.66796875e-02,  1.19628906e-01,\n",
       "        9.32617188e-02,  1.38671875e-01, -1.86523438e-01, -1.54418945e-02,\n",
       "       -5.81054688e-02,  9.76562500e-02, -5.59082031e-02, -1.04980469e-01,\n",
       "       -6.88476562e-02, -6.34765625e-02,  2.59765625e-01, -1.37695312e-01,\n",
       "       -1.55273438e-01, -7.47070312e-02,  7.76367188e-02, -5.07812500e-02,\n",
       "       -1.54296875e-01, -3.00292969e-02,  5.83496094e-02,  2.89306641e-02,\n",
       "        2.42919922e-02, -6.98242188e-02,  3.56445312e-02,  4.15039062e-02,\n",
       "       -2.47802734e-02,  1.81640625e-01,  6.15234375e-02,  2.24609375e-01,\n",
       "        8.78906250e-02,  1.51367188e-01, -1.70898438e-01,  1.01074219e-01,\n",
       "       -2.52685547e-02,  3.24707031e-02, -3.36914062e-02, -1.53198242e-02,\n",
       "       -2.64892578e-02, -1.93359375e-01, -9.42382812e-02,  6.25000000e-02,\n",
       "       -2.52685547e-02, -1.54296875e-01,  3.05175781e-03, -2.97851562e-02,\n",
       "       -9.32617188e-02, -1.80664062e-01,  3.75366211e-03,  1.68457031e-02,\n",
       "        1.55273438e-01,  1.83105469e-02, -2.95410156e-02,  1.19140625e-01,\n",
       "        7.91015625e-02, -6.88476562e-02, -8.39843750e-02,  1.63085938e-01,\n",
       "        2.48046875e-01, -1.48437500e-01, -3.66210938e-03, -1.15234375e-01,\n",
       "        1.27929688e-01, -1.90429688e-01, -1.18652344e-01,  6.25000000e-02,\n",
       "        4.54101562e-02,  7.66601562e-02, -4.71191406e-02,  2.49023438e-02,\n",
       "       -2.49023438e-02,  1.12304688e-01, -6.29882812e-02,  5.32226562e-02,\n",
       "        7.75146484e-03, -1.93359375e-01,  1.04492188e-01,  1.40625000e-01,\n",
       "       -2.41210938e-01,  2.19726562e-01, -1.63085938e-01, -1.55029297e-02,\n",
       "       -3.14453125e-01, -9.81445312e-02, -1.12304688e-02,  1.40991211e-02,\n",
       "        3.00292969e-02, -1.54296875e-01, -6.34765625e-02, -1.57226562e-01,\n",
       "       -1.11083984e-02,  1.50390625e-01, -6.05468750e-02, -1.86523438e-01,\n",
       "        6.98242188e-02, -5.63964844e-02,  2.49023438e-01,  1.29882812e-01,\n",
       "       -6.34765625e-02,  4.54101562e-02, -2.08984375e-01,  7.61718750e-02,\n",
       "       -2.61230469e-02,  8.39843750e-02, -7.42187500e-02,  1.35742188e-01,\n",
       "       -1.12304688e-01,  7.71484375e-02,  9.57031250e-02,  9.32617188e-02,\n",
       "        2.08984375e-01,  1.83593750e-01,  2.05993652e-04, -2.20947266e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"business\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23376924"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"business\",\"money\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('businesses', 0.6623775362968445),\n",
       " ('busines', 0.6080313324928284),\n",
       " ('busi_ness', 0.5612965226173401),\n",
       " ('PETER_PASSI_covers', 0.5530025959014893),\n",
       " ('Business', 0.5466139316558838),\n",
       " ('businesss', 0.5441080331802368),\n",
       " ('Sopris_supplemental_solutions', 0.5252544283866882),\n",
       " ('company', 0.5192004442214966),\n",
       " ('entrepreneurial', 0.5077816247940063),\n",
       " ('buiness', 0.5039401650428772)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"business\") #give the most similar words to \"business\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'tasti' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-69619253f802>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tasti\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# this gives ERROR because our stemmed word tasti does not exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    450\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'tasti' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv.most_similar(\"tasti\") # this gives ERROR because our stemmed word tasti does not exist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tastes', 0.6838271021842957),\n",
       " ('flavor', 0.6630197763442993),\n",
       " ('tasted', 0.6162089109420776),\n",
       " ('Harry_Potter_butterbeer', 0.589458703994751),\n",
       " ('tasting', 0.5604723691940308),\n",
       " ('tangy_taste', 0.5567916035652161),\n",
       " ('aftertaste', 0.5558385252952576),\n",
       " ('bitter_taste', 0.5491952896118164),\n",
       " ('carbonated_cough_syrup', 0.5455324649810791),\n",
       " ('taste_buds', 0.5368086099624634)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"taste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train our own Word2Vec model using our own text corpus (final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sent = []\n",
    "for sent in final_dataset[\"CleanedText\"].values:\n",
    "    list_of_sent.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "witti littl book make son laugh loud recit car drive along alway sing refrain hes learn whale india droop love new word book introduc silli classic book will bet son still abl recit memori colleg\n"
     ]
    }
   ],
   "source": [
    "print(final_dataset[\"CleanedText\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['witti', 'littl', 'book', 'make', 'son', 'laugh', 'loud', 'recit', 'car', 'drive', 'along', 'alway', 'sing', 'refrain', 'hes', 'learn', 'whale', 'india', 'droop', 'love', 'new', 'word', 'book', 'introduc', 'silli', 'classic', 'book', 'will', 'bet', 'son', 'still', 'abl', 'recit', 'memori', 'colleg']\n"
     ]
    }
   ],
   "source": [
    "print(list_of_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "perso_word2vec_model = gensim.models.Word2Vec(list_of_sent,min_count = 5,size=50,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words that appear min 5 times:  21938\n",
      "sample words [0:50]: ['deni', 'penc', 'vervein', 'rosemari', 'stretchi', 'anis', 'supermart', 'snapshot', 'benchmark', 'maloney', 'kernal', 'nuk', 'nan', 'loophol', 'boomi', 'indic', 'tastybit', 'sneek', 'canola', 'noah', 'dubbl', 'magician', 'brisk', 'pint', 'creamer', 'rasberri', 'voluptu', 'wheaten', 'thicken', 'almost', 'duller', 'kirkland', 'monstros', 'intriqu', 'afterthought', 'retent', 'maltipoo', 'wer', 'honest', 'pau', 'redmond', 'fire', 'armpit', 'remain', 'fad', 'stereo', 'watercress', 'sleepytim', 'amozon', 'vial']\n"
     ]
    }
   ],
   "source": [
    "w2v_vocab = list(perso_word2vec_model.wv.vocab)\n",
    "print(\"number of words that appear min 5 times: \",len(w2v_vocab))\n",
    "print(\"sample words [0:50]:\",w2v_vocab[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delici', 0.8074445724487305),\n",
       " ('yummi', 0.7940266728401184),\n",
       " ('tastey', 0.7560350894927979),\n",
       " ('good', 0.7038044333457947),\n",
       " ('satisfi', 0.6868366599082947),\n",
       " ('nice', 0.6649437546730042),\n",
       " ('hearti', 0.6585618853569031),\n",
       " ('nutriti', 0.6512829065322876),\n",
       " ('crunchi', 0.6447467803955078),\n",
       " ('terrif', 0.6427789926528931)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perso_word2vec_model.wv.most_similar(\"tasti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02701611, -0.10889673, -1.0262495 , -0.46574706,  0.44552073,\n",
       "       -0.8640214 , -0.85324043, -1.5809108 , -0.99243325, -0.59499544,\n",
       "        0.7233709 , -0.29276377, -1.7476803 , -0.6396086 ,  0.8851758 ,\n",
       "       -1.0584092 , -2.700595  , -1.1784486 ,  0.8550985 ,  0.5342878 ,\n",
       "        0.86521477,  0.1449451 , -2.479155  ,  0.43289283,  1.1408037 ,\n",
       "       -0.44291678, -1.304791  ,  3.484737  ,  0.49329612,  1.9975141 ,\n",
       "        1.9462025 , -0.7166388 ,  0.4743421 ,  1.2148111 , -1.1655399 ,\n",
       "        0.4190834 , -2.4263234 ,  2.6667461 , -1.2812158 , -0.44299066,\n",
       "        2.9221117 , -0.13341868,  1.4185519 , -1.6105909 , -0.6283417 ,\n",
       "        0.90079725,  4.4634657 ,  2.0880275 , -1.9010131 ,  2.3108134 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perso_word2vec_model.wv[\"tasti\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80744463"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perso_word2vec_model.wv.similarity(\"tasti\",\"delici\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aftertast', 0.770721435546875),\n",
       " ('flavor', 0.7697957754135132),\n",
       " ('tase', 0.734691858291626),\n",
       " ('flavour', 0.686099112033844),\n",
       " ('textur', 0.6777291297912598),\n",
       " ('overpow', 0.6550370454788208),\n",
       " ('overbear', 0.6543506979942322),\n",
       " ('hint', 0.6433494091033936),\n",
       " ('smell', 0.6336493492126465),\n",
       " ('underton', 0.624985933303833)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perso_word2vec_model.wv.most_similar(\"tast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg Word2Vec, TFIDF-Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-6240c21d7f62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw2v_vocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperso_word2vec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0msent_vec\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mcnt_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcnt_words\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in list_of_sent: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_vocab:\n",
    "            vec = perso_word2vec_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = tf_idf_vect.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in list_of_sent: # for each review/sentence \n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_vocab:\n",
    "            vec = perso_word2vec_model.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tf_idf = final_tfidf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
